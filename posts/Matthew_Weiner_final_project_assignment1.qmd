---
title: "Final Project Assignment#1: Matthew Weiner"
author: "Matthew Weiner"
description: "Project & Data Description"
date: "04/12/2023"
format:
  html:
    df-print: paged
    toc: true
    code-copy: true
    code-tools: true
    css: styles.css
categories:
  - final_Project_assignment_1
  - final_project_data_description
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| warning: false
#| message: false

library(tidyverse)

knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

## Part 1. Introduction {#describe-the-data-sets}

  The dataset I chose to use for this final project was  found on the website Kaggle.com. The link to the specific dataset can be found [here](https://www.kaggle.com/datasets/ppb00x/credit-risk-customers). Unfortunately the provider of the dataset did not release the source of the data, however based on the contents it is likely from some sort of credit agency. The author of this specific dataset is Jan van Rijn who is an assitant professor at Leiden University. This data was also collected starting on April 5, 2014 with no end date mentioned. 
  
  Each row in the dataset represents a customer who is attempting to get approved for some sort of credit loan. The columns all are different features of the customer which could be used to determine if the customer is a good or bad risk for this loan. 

The main question I am looking to answer about this dataset is how strong the correlation between certain features of a customer and the likelihood that that customer is classified as a good risk are. In addition, I think that the reverse (which features make a customer a bad risk) would also be very interesting to investigate.

## Part 2. Describe the data set(s) {#describe-the-data-sets-1}

```{r}
library(readr)
data <- read.csv("MatthewWeiner_FinalProjectData/credit_customers.csv")

```

```{r}
  dim(data)
```

```{r}
length(unique(data$class))
table(data$class)
```

```{r}
head(data)
```

We can now see that there are 21 variables and 1000 entries. We can also see here that there are only 2 classifications of the the credit applicant: good and bad.


We can view the summary statistics of all the columns which are of type `double` in the dataset:
```{r}
library(dplyr)
summary(select_if(data, is.double))
```

These statistics give us a good idea of the min, max, mean, and median for many of the important variables.



This dataset contains information about customers' credit applications. It has 21 columns (variables) and 1000 rows (cases) where each row represents a unique credit application. Each case contains important information about the applicant's current financial status such as their employment status, existing credits, their job, credit history, savings status, housing, and amount of property owned by the applicant. The dataset also includes more information about the applicant that isn't necessarily directly related to their financial situation such as their age, if they own a telephone, the number of dependents they have, and their current relationship status. Finally, each applicant is also given a classification in the `class` variable which classifies each person as either a good or bad risk for the credit loan agency. In this scenario, a good risk would indicated that the loan agency would be likely to provide the applicant with their loan, while a bad risk applicant would likely be rejected.


## 3. The Tentative Plan for Visualization {#the-tentative-plan-for-visualization}

I plan on conducting multiple different visualizations to answer a variety of questions. First, I plan on using a univariate visualization, likely a histogram, to plot the value of certain variables in a given class. For example, plotting the age of all applicants who were classified as a good risk could be interesting as it may show a pattern of a certain age group being more prefered than another. 

Another visualization I plan on creating would be a bivariate scatter plot. Doing so will let me visualize the relationship between many variables such as the duration of the loan vs. credit amount to see if there is a ratio of duration to amount that the agencies prefer and are likely to give a good risk classification to.

In order to conduct these analyses and visualizations, a lot of data cleaning will need to be done on this dataset. Many of the variables are not numerical and contain multiple different character values, some of which should be removed and treated as NA. I still think it would be helpful to continue to use these entries that have missing values as not having a certain feature may negatively or positively impact the applicant's success rate. There are many variables that the values have unclear/messy names so it will be helpful for clarity to change those value names to something more readable to the viewers. At this point, I do not expect that any pivoting will need to be done to this data format. 
